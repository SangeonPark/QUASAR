{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import itertools\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch.utils.data as utils\n",
    "import math\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from argparse import ArgumentParser\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "from flows import RealNVP, Planar\n",
    "from models import NormalizingFlowModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_PureBkg = pd.read_hdf(\"/data/t3home000/spark/LHCOlympics/data/MassRatio_pureBkg.h5\")\n",
    "dt_PureBkg = f_PureBkg.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_PureBkg[:,1] = (dt_PureBkg[:,1]-np.mean(dt_PureBkg[:,1]))/np.std(dt_PureBkg[:,1])\n",
    "dt_PureBkg[:,2] = (dt_PureBkg[:,2]-np.mean(dt_PureBkg[:,2]))/np.std(dt_PureBkg[:,2])\n",
    "dt_PureBkg[:,3] = (dt_PureBkg[:,3]-np.mean(dt_PureBkg[:,3]))/np.std(dt_PureBkg[:,3])\n",
    "dt_PureBkg[:,4] = (dt_PureBkg[:,4]-np.mean(dt_PureBkg[:,4]))/np.std(dt_PureBkg[:,4])\n",
    "dt_PureBkg[:,5] = (dt_PureBkg[:,5]-np.mean(dt_PureBkg[:,5]))/np.std(dt_PureBkg[:,5])\n",
    "dt_PureBkg[:,6] = (dt_PureBkg[:,6]-np.mean(dt_PureBkg[:,6]))/np.std(dt_PureBkg[:,6])\n",
    "\n",
    "\n",
    "dt_PureBkg[:,8] = (dt_PureBkg[:,8]-np.mean(dt_PureBkg[:,8]))/np.std(dt_PureBkg[:,8])\n",
    "dt_PureBkg[:,9] = (dt_PureBkg[:,9]-np.mean(dt_PureBkg[:,9]))/np.std(dt_PureBkg[:,9])\n",
    "dt_PureBkg[:,10] = (dt_PureBkg[:,10]-np.mean(dt_PureBkg[:,10]))/np.std(dt_PureBkg[:,10])\n",
    "dt_PureBkg[:,11] = (dt_PureBkg[:,11]-np.mean(dt_PureBkg[:,11]))/np.std(dt_PureBkg[:,11])\n",
    "dt_PureBkg[:,12] = (dt_PureBkg[:,12]-np.mean(dt_PureBkg[:,12]))/np.std(dt_PureBkg[:,12])\n",
    "\n",
    "dt_PureBkg[:,14] = (dt_PureBkg[:,14]-np.mean(dt_PureBkg[:,14]))/np.std(dt_PureBkg[:,14])\n",
    "dt_PureBkg[:,15] = (dt_PureBkg[:,15]-np.mean(dt_PureBkg[:,15]))/np.std(dt_PureBkg[:,15])\n",
    "dt_PureBkg[:,16] = (dt_PureBkg[:,16]-np.mean(dt_PureBkg[:,16]))/np.std(dt_PureBkg[:,16])\n",
    "dt_PureBkg[:,17] = (dt_PureBkg[:,17]-np.mean(dt_PureBkg[:,17]))/np.std(dt_PureBkg[:,17])\n",
    "dt_PureBkg[:,18] = (dt_PureBkg[:,18]-np.mean(dt_PureBkg[:,18]))/np.std(dt_PureBkg[:,18])\n",
    "dt_PureBkg[:,19] = (dt_PureBkg[:,19]-np.mean(dt_PureBkg[:,19]))/np.std(dt_PureBkg[:,19])\n",
    "\n",
    "\n",
    "dt_PureBkg[:,21] = (dt_PureBkg[:,21]-np.mean(dt_PureBkg[:,21]))/np.std(dt_PureBkg[:,21])\n",
    "dt_PureBkg[:,22] = (dt_PureBkg[:,22]-np.mean(dt_PureBkg[:,22]))/np.std(dt_PureBkg[:,22])\n",
    "dt_PureBkg[:,23] = (dt_PureBkg[:,23]-np.mean(dt_PureBkg[:,23]))/np.std(dt_PureBkg[:,23])\n",
    "dt_PureBkg[:,24] = (dt_PureBkg[:,24]-np.mean(dt_PureBkg[:,24]))/np.std(dt_PureBkg[:,24])\n",
    "dt_PureBkg[:,25] = (dt_PureBkg[:,25]-np.mean(dt_PureBkg[:,25]))/np.std(dt_PureBkg[:,25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_PureBkg = torch.tensor(dt_PureBkg)\n",
    "total_PureBkg_train_x_1 = total_PureBkg.t()[1:7].t()\n",
    "total_PureBkg_train_x_2 = total_PureBkg.t()[8:13].t()\n",
    "total_PureBkg_train_x_3 = total_PureBkg.t()[14:20].t()\n",
    "total_PureBkg_train_x_4 = total_PureBkg.t()[21:26].t()\n",
    "\n",
    "total_PureBkg_selection = torch.cat((total_PureBkg_train_x_1,total_PureBkg_train_x_2,total_PureBkg_train_x_3,total_PureBkg_train_x_4),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1000\n",
    "bkgAE_train_iterator = utils.DataLoader(total_PureBkg_selection, batch_size=bs, shuffle=True) \n",
    "bkgAE_test_iterator = utils.DataLoader(total_PureBkg_selection, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_NF(nn.Module):\n",
    "    def __init__(self, K, D):\n",
    "        super().__init__()\n",
    "        self.dim = D\n",
    "        self.K = K\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(22, 96),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(96, 48),\n",
    "            nn.LeakyReLU(True),            \n",
    "            nn.Linear(48, D * 2)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(D, 48),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(48, 96),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(96, 22),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        flow_init = RealNVP(dim=D)\n",
    "        flows_init = [flow_init for _ in range(K)]\n",
    "        prior = MultivariateNormal(torch.zeros(D).cuda(), torch.eye(D).cuda())\n",
    "        self.flows = NormalizingFlowModel(prior, flows_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run Encoder and get NF params\n",
    "        enc = self.encoder(x)\n",
    "        mu = enc[:, :self.dim]\n",
    "        log_var = enc[:, self.dim: self.dim * 2]\n",
    "\n",
    "        # Re-parametrize\n",
    "        sigma = (log_var * .5).exp()\n",
    "        z = mu + sigma * torch.randn_like(sigma)\n",
    "        kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        # Construct more expressive posterior with NF\n",
    "        \n",
    "        z_k, _, sum_ladj = self.flows(z)\n",
    "        \n",
    "        kl_div = kl_div / x.size(0) - sum_ladj.mean()  # mean over batch\n",
    "\n",
    "        # Run Decoder\n",
    "        x_prime = self.decoder(z_k)\n",
    "        return x_prime, kl_div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Instance¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 20\n",
    "PRINT_INTERVAL = 2000\n",
    "NUM_WORKERS = 4\n",
    "LR = 1e-4\n",
    "\n",
    "N_FLOWS = 4\n",
    "Z_DIM = 4\n",
    "\n",
    "n_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE_NF(N_FLOWS, Z_DIM).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    global n_steps\n",
    "    train_loss = []\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, x in enumerate(bkgAE_train_iterator):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        x = x.float().cuda()\n",
    "\n",
    "        x_tilde, kl_div = model(x)\n",
    "        loss_recons = F.binary_cross_entropy(x_tilde, x, size_average=False) / x.size(0)\n",
    "        loss = loss_recons + kl_div\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append([loss_recons.item(), kl_div.item()])\n",
    "\n",
    "        if (batch_idx + 1) % PRINT_INTERVAL == 0:\n",
    "            print('\\tIter [{}/{} ({:.0f}%)]\\tLoss: {} Time: {:5.3f} ms/batch'.format(\n",
    "                batch_idx * len(x), 50000,\n",
    "                PRINT_INTERVAL * batch_idx / 50000,\n",
    "                np.asarray(train_loss)[-PRINT_INTERVAL:].mean(0),\n",
    "                1000 * (time.time() - start_time)\n",
    "            ))\n",
    "\n",
    "        n_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(split='valid'):\n",
    "    global n_steps\n",
    "    start_time = time.time()\n",
    "    val_loss = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, x in enumerate(bkgAE_test_iterator):\n",
    "            \n",
    "            x = x.float().cuda()\n",
    "\n",
    "            x_tilde, kl_div = model(x)\n",
    "            loss_recons = F.binary_cross_entropy(x_tilde, x, size_average=False) / x.size(0)\n",
    "            loss = loss_recons + kl_div\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "    print('\\nEvaluation Completed ({})!\\tLoss: {:5.4f} Time: {:5.3f} s'.format(\n",
    "        split,\n",
    "        np.asarray(val_loss).mean(0),\n",
    "        time.time() - start_time\n",
    "    ))\n",
    "    return np.asarray(val_loss).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark/miniconda3/envs/myenv/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Completed (valid)!\tLoss: -406.9379 Time: 6.911 s\n",
      "Saving model!\n",
      "Epoch 2:\n",
      "\n",
      "Evaluation Completed (valid)!\tLoss: -395.7585 Time: 6.974 s\n",
      "Not saving model! Last saved: 1\n",
      "Epoch 3:\n",
      "\n",
      "Evaluation Completed (valid)!\tLoss: -395.8453 Time: 6.955 s\n",
      "Not saving model! Last saved: 1\n",
      "Epoch 4:\n",
      "\n",
      "Evaluation Completed (valid)!\tLoss: -392.4381 Time: 6.917 s\n",
      "Not saving model! Last saved: 1\n",
      "Epoch 5:\n",
      "\n",
      "Evaluation Completed (valid)!\tLoss: -392.1311 Time: 7.056 s\n",
      "Not saving model! Last saved: 1\n",
      "Epoch 6:\n",
      "\n",
      "Evaluation Completed (valid)!\tLoss: -393.9002 Time: 7.014 s\n",
      "Not saving model! Last saved: 1\n",
      "Epoch 7:\n",
      "\n",
      "Evaluation Completed (valid)!\tLoss: -390.4122 Time: 7.168 s\n",
      "Not saving model! Last saved: 1\n",
      "Epoch 8:\n",
      "\n",
      "Evaluation Completed (valid)!\tLoss: -384.8078 Time: 7.104 s\n",
      "Not saving model! Last saved: 1\n",
      "Epoch 9:\n",
      "\n",
      "Evaluation Completed (valid)!\tLoss: -389.6537 Time: 6.890 s\n",
      "Not saving model! Last saved: 1\n",
      "Epoch 10:\n",
      "\n",
      "Evaluation Completed (valid)!\tLoss: -379.5034 Time: 6.879 s\n",
      "Not saving model! Last saved: 1\n",
      "Epoch 11:\n",
      "\n",
      "Evaluation Completed (valid)!\tLoss: -392.3933 Time: 6.918 s\n",
      "Not saving model! Last saved: 1\n",
      "Epoch 12:\n",
      "\n",
      "Evaluation Completed (valid)!\tLoss: -390.4629 Time: 6.916 s\n",
      "Not saving model! Last saved: 1\n",
      "Epoch 13:\n",
      "\n",
      "Evaluation Completed (valid)!\tLoss: -384.5843 Time: 7.065 s\n",
      "Not saving model! Last saved: 1\n",
      "Epoch 14:\n",
      "\n",
      "Evaluation Completed (valid)!\tLoss: -388.3176 Time: 7.209 s\n",
      "Not saving model! Last saved: 1\n",
      "Epoch 15:\n",
      "\n",
      "Evaluation Completed (valid)!\tLoss: -381.3199 Time: 7.096 s\n",
      "Not saving model! Last saved: 1\n",
      "Epoch 16:\n",
      "\n",
      "Evaluation Completed (valid)!\tLoss: -379.2883 Time: 6.872 s\n",
      "Not saving model! Last saved: 1\n",
      "Epoch 17:\n",
      "\n",
      "Evaluation Completed (valid)!\tLoss: -387.8420 Time: 6.848 s\n",
      "Not saving model! Last saved: 1\n",
      "Epoch 18:\n",
      "\n",
      "Evaluation Completed (valid)!\tLoss: -389.7746 Time: 6.834 s\n",
      "Not saving model! Last saved: 1\n",
      "Epoch 19:\n",
      "\n",
      "Evaluation Completed (valid)!\tLoss: -382.3597 Time: 8.221 s\n",
      "Not saving model! Last saved: 1\n"
     ]
    }
   ],
   "source": [
    "BEST_LOSS = 99999\n",
    "LAST_SAVED = -1\n",
    "for epoch in range(1, N_EPOCHS):\n",
    "    print(\"Epoch {}:\".format(epoch))\n",
    "    train()\n",
    "    cur_loss = evaluate()\n",
    "\n",
    "    if cur_loss <= BEST_LOSS:\n",
    "        BEST_LOSS = cur_loss\n",
    "        LAST_SAVED = epoch\n",
    "        print(\"Saving model!\")\n",
    "        torch.save(model.state_dict(), \"/data/t3home000/spark/QUASAR/weights/bkg_vae_NF_NVP.h5\")\n",
    "    else:\n",
    "        print(\"Not saving model! Last saved: {}\".format(LAST_SAVED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(dt_in):\n",
    "\n",
    "    dt_in[:,1] = (dt_in[:,1]-np.mean(dt_in[:,1]))/np.std(dt_in[:,1])\n",
    "    dt_in[:,2] = (dt_in[:,2]-np.mean(dt_in[:,2]))/np.std(dt_in[:,2])\n",
    "    dt_in[:,3] = (dt_in[:,3]-np.mean(dt_in[:,3]))/np.std(dt_in[:,3])\n",
    "    dt_in[:,4] = (dt_in[:,4]-np.mean(dt_in[:,4]))/np.std(dt_in[:,4])\n",
    "    dt_in[:,5] = (dt_in[:,5]-np.mean(dt_in[:,5]))/np.std(dt_in[:,5])\n",
    "    dt_in[:,6] = (dt_in[:,6]-np.mean(dt_in[:,6]))/np.std(dt_in[:,6])\n",
    "\n",
    "    dt_in[:,8] = (dt_in[:,8]-np.mean(dt_in[:,8]))/np.std(dt_in[:,8])\n",
    "    dt_in[:,9] = (dt_in[:,9]-np.mean(dt_in[:,9]))/np.std(dt_in[:,9])\n",
    "    dt_in[:,10] = (dt_in[:,10]-np.mean(dt_in[:,10]))/np.std(dt_in[:,10])\n",
    "    dt_in[:,11] = (dt_in[:,11]-np.mean(dt_in[:,11]))/np.std(dt_in[:,11])\n",
    "    dt_in[:,12] = (dt_in[:,12]-np.mean(dt_in[:,12]))/np.std(dt_in[:,12])\n",
    "\n",
    "    dt_in[:,14] = (dt_in[:,14]-np.mean(dt_in[:,14]))/np.std(dt_in[:,14])\n",
    "    dt_in[:,15] = (dt_in[:,15]-np.mean(dt_in[:,15]))/np.std(dt_in[:,15])\n",
    "    dt_in[:,16] = (dt_in[:,16]-np.mean(dt_in[:,16]))/np.std(dt_in[:,16])\n",
    "    dt_in[:,17] = (dt_in[:,17]-np.mean(dt_in[:,17]))/np.std(dt_in[:,17])\n",
    "    dt_in[:,18] = (dt_in[:,18]-np.mean(dt_in[:,18]))/np.std(dt_in[:,18])\n",
    "    dt_in[:,19] = (dt_in[:,19]-np.mean(dt_in[:,19]))/np.std(dt_in[:,19])\n",
    "    \n",
    "    dt_in[:,21] = (dt_in[:,21]-np.mean(dt_in[:,21]))/np.std(dt_in[:,21])\n",
    "    dt_in[:,22] = (dt_in[:,22]-np.mean(dt_in[:,22]))/np.std(dt_in[:,22])\n",
    "    dt_in[:,23] = (dt_in[:,23]-np.mean(dt_in[:,23]))/np.std(dt_in[:,23])\n",
    "    dt_in[:,24] = (dt_in[:,24]-np.mean(dt_in[:,24]))/np.std(dt_in[:,24])\n",
    "    dt_in[:,25] = (dt_in[:,25]-np.mean(dt_in[:,25]))/np.std(dt_in[:,25])   \n",
    "    \n",
    "    total_in = torch.tensor(dt_in)\n",
    "    total_in_train_x_1 = total_in.t()[1:7].t()\n",
    "    total_in_train_x_2 = total_in.t()[8:13].t()\n",
    "    total_in_train_x_3 = total_in.t()[14:20].t()\n",
    "    total_in_train_x_4 = total_in.t()[21:26].t()\n",
    "    total_in_selection = torch.cat((total_in_train_x_1,total_in_train_x_2,total_in_train_x_3,total_in_train_x_4),dim=1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loss_total_in = torch.mean((model(total_in_selection.float().cuda())[0]- total_in_selection.float().cuda())**2,dim=1).data.cpu().numpy()\n",
    "    \n",
    "    return loss_total_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mass(inputstring):\n",
    "\n",
    "    f_in = pd.read_hdf(inputstring)\n",
    "    dt_in = f_in.values\n",
    "    \n",
    "    return dt_in[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File ../../../2_lhc/LHC_Olympics2020/processing/test_dataset/MassRatio_BB1.h5 does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-99f826ce2c8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbb2mass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../../2_lhc/LHC_Olympics2020/processing/test_dataset/MassRatio_BB1.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpurebkgmass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../../2_lhc/LHC_Olympics2020/processing/test_dataset/MassRatio_pureBkg.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-5fb1e6cd4f5d>\u001b[0m in \u001b[0;36mget_mass\u001b[0;34m(inputstring)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_mass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mf_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdt_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.6/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mread_hdf\u001b[0;34m(path_or_buf, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"File {path_or_buf} does not exist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0mstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHDFStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File ../../../2_lhc/LHC_Olympics2020/processing/test_dataset/MassRatio_BB1.h5 does not exist"
     ]
    }
   ],
   "source": [
    "bb2mass = get_mass(\"../../../2_lhc/LHC_Olympics2020/processing/test_dataset/MassRatio_BB1.h5\")\n",
    "purebkgmass = get_mass(\"../../../2_lhc/LHC_Olympics2020/processing/test_dataset/MassRatio_pureBkg.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb2mass, bb2mmdt1, bb2mmdt2, bb2prun1,bb2prun2, bb2loss = get_mass_and_loss(\"../../../2_lhc/LHC_Olympics2020/processing/test_dataset/MassRatio_BB1.h5\")\n",
    "purebkgmass, purebkgmmdt1, purebkgmmdt2, purebkgprun1,purebkgprun2, purebkgloss = get_mass_and_loss(\"../../../2_lhc/LHC_Olympics2020/processing/test_dataset/MassRatio_pureBkg.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "bins = np.linspace(0,5,1100)\n",
    "plt.hist(bb2loss,bins=bins,alpha=0.3,color='b',label='blackbox1')\n",
    "plt.hist(purebkgloss,bins=bins,alpha=0.3,color='r',label='background')\n",
    "plt.xlabel(r'Autoencoder Loss')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.read_hdf(\"/data/t3home000/spark/LHCOlympics/data/MassRatio_RandD.h5\")\n",
    "dt = f.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mjj</th>\n",
       "      <th>Mj1</th>\n",
       "      <th>j1 tau21</th>\n",
       "      <th>j1 tau32</th>\n",
       "      <th>j1 tau43</th>\n",
       "      <th>j1 sqrt(tau^2_1)/tau^1_1</th>\n",
       "      <th>j1 n_trk</th>\n",
       "      <th>j1 pT1</th>\n",
       "      <th>j1 M_trim</th>\n",
       "      <th>j1 M_prun</th>\n",
       "      <th>...</th>\n",
       "      <th>j2 sqrt(tau^2_1)/tau^1_1</th>\n",
       "      <th>j2 n_trk</th>\n",
       "      <th>j2 pT1</th>\n",
       "      <th>j2 M_trim</th>\n",
       "      <th>j2 M_prun</th>\n",
       "      <th>j2 M_mmdt</th>\n",
       "      <th>j2 M_sdb1</th>\n",
       "      <th>j2 M_sdb2</th>\n",
       "      <th>j2 M_sdm1</th>\n",
       "      <th>isSignal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2577.571899</td>\n",
       "      <td>98.677270</td>\n",
       "      <td>0.528903</td>\n",
       "      <td>0.788281</td>\n",
       "      <td>0.904471</td>\n",
       "      <td>4.241889</td>\n",
       "      <td>136.0</td>\n",
       "      <td>1285.895950</td>\n",
       "      <td>18.881765</td>\n",
       "      <td>9.797733</td>\n",
       "      <td>...</td>\n",
       "      <td>1.895988</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1282.286017</td>\n",
       "      <td>42.162664</td>\n",
       "      <td>18.466533</td>\n",
       "      <td>18.466533</td>\n",
       "      <td>31.845136</td>\n",
       "      <td>42.162664</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3807.507389</td>\n",
       "      <td>584.595432</td>\n",
       "      <td>0.345626</td>\n",
       "      <td>0.463461</td>\n",
       "      <td>0.865982</td>\n",
       "      <td>1.069972</td>\n",
       "      <td>320.0</td>\n",
       "      <td>1334.493332</td>\n",
       "      <td>556.665923</td>\n",
       "      <td>562.607897</td>\n",
       "      <td>...</td>\n",
       "      <td>1.377217</td>\n",
       "      <td>348.0</td>\n",
       "      <td>1306.137883</td>\n",
       "      <td>395.226881</td>\n",
       "      <td>393.309512</td>\n",
       "      <td>405.034096</td>\n",
       "      <td>405.034096</td>\n",
       "      <td>405.034096</td>\n",
       "      <td>405.034096</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1710.965414</td>\n",
       "      <td>159.597526</td>\n",
       "      <td>0.677692</td>\n",
       "      <td>0.690707</td>\n",
       "      <td>0.695322</td>\n",
       "      <td>1.310040</td>\n",
       "      <td>332.0</td>\n",
       "      <td>678.557182</td>\n",
       "      <td>144.351550</td>\n",
       "      <td>142.366275</td>\n",
       "      <td>...</td>\n",
       "      <td>1.887494</td>\n",
       "      <td>236.0</td>\n",
       "      <td>1072.462085</td>\n",
       "      <td>54.235070</td>\n",
       "      <td>41.967840</td>\n",
       "      <td>41.352112</td>\n",
       "      <td>51.721630</td>\n",
       "      <td>70.442364</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2603.379037</td>\n",
       "      <td>515.237299</td>\n",
       "      <td>0.091038</td>\n",
       "      <td>0.784454</td>\n",
       "      <td>0.860716</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>248.0</td>\n",
       "      <td>1284.020224</td>\n",
       "      <td>501.564320</td>\n",
       "      <td>506.727622</td>\n",
       "      <td>...</td>\n",
       "      <td>1.997360</td>\n",
       "      <td>352.0</td>\n",
       "      <td>1217.031950</td>\n",
       "      <td>81.842001</td>\n",
       "      <td>60.307703</td>\n",
       "      <td>60.307703</td>\n",
       "      <td>72.423677</td>\n",
       "      <td>84.480859</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3294.162200</td>\n",
       "      <td>142.420213</td>\n",
       "      <td>0.507714</td>\n",
       "      <td>0.522686</td>\n",
       "      <td>0.904070</td>\n",
       "      <td>1.853319</td>\n",
       "      <td>220.0</td>\n",
       "      <td>1087.658980</td>\n",
       "      <td>129.146700</td>\n",
       "      <td>36.160229</td>\n",
       "      <td>...</td>\n",
       "      <td>1.113248</td>\n",
       "      <td>204.0</td>\n",
       "      <td>1205.343324</td>\n",
       "      <td>103.456059</td>\n",
       "      <td>99.817788</td>\n",
       "      <td>103.456059</td>\n",
       "      <td>103.456059</td>\n",
       "      <td>103.456059</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Mjj         Mj1  j1 tau21  j1 tau32  j1 tau43  \\\n",
       "0  2577.571899   98.677270  0.528903  0.788281  0.904471   \n",
       "1  3807.507389  584.595432  0.345626  0.463461  0.865982   \n",
       "2  1710.965414  159.597526  0.677692  0.690707  0.695322   \n",
       "3  2603.379037  515.237299  0.091038  0.784454  0.860716   \n",
       "4  3294.162200  142.420213  0.507714  0.522686  0.904070   \n",
       "\n",
       "   j1 sqrt(tau^2_1)/tau^1_1  j1 n_trk       j1 pT1   j1 M_trim   j1 M_prun  \\\n",
       "0                  4.241889     136.0  1285.895950   18.881765    9.797733   \n",
       "1                  1.069972     320.0  1334.493332  556.665923  562.607897   \n",
       "2                  1.310040     332.0   678.557182  144.351550  142.366275   \n",
       "3                  1.102743     248.0  1284.020224  501.564320  506.727622   \n",
       "4                  1.853319     220.0  1087.658980  129.146700   36.160229   \n",
       "\n",
       "   ...  j2 sqrt(tau^2_1)/tau^1_1  j2 n_trk       j2 pT1   j2 M_trim  \\\n",
       "0  ...                  1.895988     128.0  1282.286017   42.162664   \n",
       "1  ...                  1.377217     348.0  1306.137883  395.226881   \n",
       "2  ...                  1.887494     236.0  1072.462085   54.235070   \n",
       "3  ...                  1.997360     352.0  1217.031950   81.842001   \n",
       "4  ...                  1.113248     204.0  1205.343324  103.456059   \n",
       "\n",
       "    j2 M_prun   j2 M_mmdt   j2 M_sdb1   j2 M_sdb2   j2 M_sdm1  isSignal  \n",
       "0   18.466533   18.466533   31.845136   42.162664    0.000000       0.0  \n",
       "1  393.309512  405.034096  405.034096  405.034096  405.034096       0.0  \n",
       "2   41.967840   41.352112   51.721630   70.442364   -0.000003       0.0  \n",
       "3   60.307703   60.307703   72.423677   84.480859    0.000003       0.0  \n",
       "4   99.817788  103.456059  103.456059  103.456059    0.000008       1.0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = dt[:,27]\n",
    "bkg_idx = np.where(idx==0)[0]\n",
    "signal_idx = np.where(idx==1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(idx[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mass_and_loss(inputstring):\n",
    "    f_in = pd.read_hdf(inputstring)\n",
    "    dt_in = f_in.values\n",
    "    dt_in[:,1] = (dt_in[:,1]-np.mean(dt_in[:,1]))/np.std(dt_in[:,1])\n",
    "    dt_in[:,2] = (dt_in[:,2]-np.mean(dt_in[:,2]))/np.std(dt_in[:,2])\n",
    "    dt_in[:,3] = (dt_in[:,3]-np.mean(dt_in[:,3]))/np.std(dt_in[:,3])\n",
    "    dt_in[:,4] = (dt_in[:,4]-np.mean(dt_in[:,4]))/np.std(dt_in[:,4])\n",
    "    dt_in[:,5] = (dt_in[:,5]-np.mean(dt_in[:,5]))/np.std(dt_in[:,5])\n",
    "    dt_in[:,6] = (dt_in[:,6]-np.mean(dt_in[:,6]))/np.std(dt_in[:,6])\n",
    "\n",
    "    dt_in[:,8] = (dt_in[:,8]-np.mean(dt_in[:,8]))/np.std(dt_in[:,8])\n",
    "    dt_in[:,9] = (dt_in[:,9]-np.mean(dt_in[:,9]))/np.std(dt_in[:,9])\n",
    "    dt_in[:,10] = (dt_in[:,10]-np.mean(dt_in[:,10]))/np.std(dt_in[:,10])\n",
    "    dt_in[:,11] = (dt_in[:,11]-np.mean(dt_in[:,11]))/np.std(dt_in[:,11])\n",
    "    dt_in[:,12] = (dt_in[:,12]-np.mean(dt_in[:,12]))/np.std(dt_in[:,12])\n",
    "\n",
    "    dt_in[:,14] = (dt_in[:,14]-np.mean(dt_in[:,14]))/np.std(dt_in[:,14])\n",
    "    dt_in[:,15] = (dt_in[:,15]-np.mean(dt_in[:,15]))/np.std(dt_in[:,15])\n",
    "    dt_in[:,16] = (dt_in[:,16]-np.mean(dt_in[:,16]))/np.std(dt_in[:,16])\n",
    "    dt_in[:,17] = (dt_in[:,17]-np.mean(dt_in[:,17]))/np.std(dt_in[:,17])\n",
    "    dt_in[:,18] = (dt_in[:,18]-np.mean(dt_in[:,18]))/np.std(dt_in[:,18])\n",
    "    dt_in[:,19] = (dt_in[:,19]-np.mean(dt_in[:,19]))/np.std(dt_in[:,19])\n",
    "    \n",
    "    dt_in[:,21] = (dt_in[:,21]-np.mean(dt_in[:,21]))/np.std(dt_in[:,21])\n",
    "    dt_in[:,22] = (dt_in[:,22]-np.mean(dt_in[:,22]))/np.std(dt_in[:,22])\n",
    "    dt_in[:,23] = (dt_in[:,23]-np.mean(dt_in[:,23]))/np.std(dt_in[:,23])\n",
    "    dt_in[:,24] = (dt_in[:,24]-np.mean(dt_in[:,24]))/np.std(dt_in[:,24])\n",
    "    dt_in[:,25] = (dt_in[:,25]-np.mean(dt_in[:,25]))/np.std(dt_in[:,25])   \n",
    "    \n",
    "    total_in = torch.tensor(dt_in)\n",
    "    total_in_train_x_1 = total_in.t()[1:7].t()\n",
    "    total_in_train_x_2 = total_in.t()[8:13].t()\n",
    "    total_in_train_x_3 = total_in.t()[14:20].t()\n",
    "    total_in_train_x_4 = total_in.t()[21:26].t()\n",
    "    total_in_selection = torch.cat((total_in_train_x_1,total_in_train_x_2,total_in_train_x_3,total_in_train_x_4),dim=1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loss_total_in = torch.mean((model(total_in_selection.float().cuda())[0]- total_in_selection.float().cuda())**2,dim=1).data.cpu().numpy()\n",
    "    \n",
    "    f_in = pd.read_hdf(inputstring)\n",
    "    dt_in = f_in.values\n",
    "    \n",
    "    return dt_in[:,0], dt_in[:,10], dt_in[:,23], dt_in[:,9], dt_in[:,22], loss_total_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb2mass, bb2mmdt1, bb2mmdt2, bb2prun1,bb2prun2, bb2loss = get_mass_and_loss(\"../../../2_lhc/LHC_Olympics2020/processing/test_dataset/MassRatio_BB1.h5\")\n",
    "purebkgmass, purebkgmmdt1, purebkgmmdt2, purebkgprun1,purebkgprun2, purebkgloss = get_mass_and_loss(\"../../../2_lhc/LHC_Olympics2020/processing/test_dataset/MassRatio_pureBkg.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bkg = torch.tensor(dt[bkg_idx])\n",
    "data_signal = torch.tensor(dt[signal_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_x_1 = data_bkg.t()[0:6].t()\n",
    "data_train_x_2 = data_bkg.t()[7:13].t()\n",
    "data_test_bkg = torch.cat((data_train_x_1,data_train_x_2),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_x_1 = data_signal.t()[0:6].t()\n",
    "data_train_x_2 = data_signal.t()[7:13].t()\n",
    "data_test_signal = torch.cat((data_train_x_1,data_train_x_2),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_bkg = get_loss(dt[bkg_idx,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_sig = get_loss(dt[signal_idx,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tpr_fpr(sigloss,bkgloss,aetype='sig'):\n",
    "    bins = np.linspace(0,50,1001)\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    for cut in bins:\n",
    "        if aetype == 'sig':\n",
    "            tpr.append(np.where(sigloss<cut)[0].shape[0]/len(sigloss))\n",
    "            fpr.append(np.where(bkgloss<cut)[0].shape[0]/len(bkgloss))\n",
    "        if aetype == 'bkg':\n",
    "            tpr.append(np.where(sigloss>cut)[0].shape[0]/len(sigloss))\n",
    "            fpr.append(np.where(bkgloss>cut)[0].shape[0]/len(bkgloss))\n",
    "    return tpr,fpr        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_tpr, bkg_fpr = get_tpr_fpr(loss_sig,loss_bkg,aetype='bkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('NFLOWVAE_NVP_bkgAE_fpr.npy',bkg_fpr)\n",
    "np.save('NFLOWVAE_NVP_bkgAE_tpr.npy',bkg_tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
