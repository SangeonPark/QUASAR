{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import itertools\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch.utils.data as utils\n",
    "import math\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from argparse import ArgumentParser\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "from flows import RealNVP, Planar, MAF, Radial\n",
    "from models import NormalizingFlowModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'ROC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f_rnd = pd.read_hdf(\"/data/t3home000/spark/LHCOlympics/data/MassRatio_RandD.h5\")\n",
    "f_PureBkg = pd.read_hdf(\"/data/t3home000/spark/LHCOlympics/data/MassRatio_pureBkg.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mjj</th>\n",
       "      <th>Mj1</th>\n",
       "      <th>j1 tau21</th>\n",
       "      <th>j1 tau32</th>\n",
       "      <th>j1 tau43</th>\n",
       "      <th>j1 sqrt(tau^2_1)/tau^1_1</th>\n",
       "      <th>j1 n_trk</th>\n",
       "      <th>j1 pT1</th>\n",
       "      <th>j1 M_trim</th>\n",
       "      <th>j1 M_prun</th>\n",
       "      <th>...</th>\n",
       "      <th>j2 sqrt(tau^2_1)/tau^1_1</th>\n",
       "      <th>j2 n_trk</th>\n",
       "      <th>j2 pT1</th>\n",
       "      <th>j2 M_trim</th>\n",
       "      <th>j2 M_prun</th>\n",
       "      <th>j2 M_mmdt</th>\n",
       "      <th>j2 M_sdb1</th>\n",
       "      <th>j2 M_sdb2</th>\n",
       "      <th>j2 M_sdm1</th>\n",
       "      <th>isSignal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2577.571899</td>\n",
       "      <td>98.677270</td>\n",
       "      <td>0.528903</td>\n",
       "      <td>0.788281</td>\n",
       "      <td>0.904471</td>\n",
       "      <td>4.241889</td>\n",
       "      <td>136.0</td>\n",
       "      <td>1285.895950</td>\n",
       "      <td>18.881765</td>\n",
       "      <td>9.797733</td>\n",
       "      <td>...</td>\n",
       "      <td>1.895988</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1282.286017</td>\n",
       "      <td>42.162664</td>\n",
       "      <td>18.466533</td>\n",
       "      <td>18.466533</td>\n",
       "      <td>31.845136</td>\n",
       "      <td>42.162664</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3807.507389</td>\n",
       "      <td>584.595432</td>\n",
       "      <td>0.345626</td>\n",
       "      <td>0.463461</td>\n",
       "      <td>0.865982</td>\n",
       "      <td>1.069972</td>\n",
       "      <td>320.0</td>\n",
       "      <td>1334.493332</td>\n",
       "      <td>556.665923</td>\n",
       "      <td>562.607897</td>\n",
       "      <td>...</td>\n",
       "      <td>1.377217</td>\n",
       "      <td>348.0</td>\n",
       "      <td>1306.137883</td>\n",
       "      <td>395.226881</td>\n",
       "      <td>393.309512</td>\n",
       "      <td>405.034096</td>\n",
       "      <td>405.034096</td>\n",
       "      <td>405.034096</td>\n",
       "      <td>405.034096</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1710.965414</td>\n",
       "      <td>159.597526</td>\n",
       "      <td>0.677692</td>\n",
       "      <td>0.690707</td>\n",
       "      <td>0.695322</td>\n",
       "      <td>1.310040</td>\n",
       "      <td>332.0</td>\n",
       "      <td>678.557182</td>\n",
       "      <td>144.351550</td>\n",
       "      <td>142.366275</td>\n",
       "      <td>...</td>\n",
       "      <td>1.887494</td>\n",
       "      <td>236.0</td>\n",
       "      <td>1072.462085</td>\n",
       "      <td>54.235070</td>\n",
       "      <td>41.967840</td>\n",
       "      <td>41.352112</td>\n",
       "      <td>51.721630</td>\n",
       "      <td>70.442364</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2603.379037</td>\n",
       "      <td>515.237299</td>\n",
       "      <td>0.091038</td>\n",
       "      <td>0.784454</td>\n",
       "      <td>0.860716</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>248.0</td>\n",
       "      <td>1284.020224</td>\n",
       "      <td>501.564320</td>\n",
       "      <td>506.727622</td>\n",
       "      <td>...</td>\n",
       "      <td>1.997360</td>\n",
       "      <td>352.0</td>\n",
       "      <td>1217.031950</td>\n",
       "      <td>81.842001</td>\n",
       "      <td>60.307703</td>\n",
       "      <td>60.307703</td>\n",
       "      <td>72.423677</td>\n",
       "      <td>84.480859</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3294.162200</td>\n",
       "      <td>142.420213</td>\n",
       "      <td>0.507714</td>\n",
       "      <td>0.522686</td>\n",
       "      <td>0.904070</td>\n",
       "      <td>1.853319</td>\n",
       "      <td>220.0</td>\n",
       "      <td>1087.658980</td>\n",
       "      <td>129.146700</td>\n",
       "      <td>36.160229</td>\n",
       "      <td>...</td>\n",
       "      <td>1.113248</td>\n",
       "      <td>204.0</td>\n",
       "      <td>1205.343324</td>\n",
       "      <td>103.456059</td>\n",
       "      <td>99.817788</td>\n",
       "      <td>103.456059</td>\n",
       "      <td>103.456059</td>\n",
       "      <td>103.456059</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Mjj         Mj1  j1 tau21  j1 tau32  j1 tau43  \\\n",
       "0  2577.571899   98.677270  0.528903  0.788281  0.904471   \n",
       "1  3807.507389  584.595432  0.345626  0.463461  0.865982   \n",
       "2  1710.965414  159.597526  0.677692  0.690707  0.695322   \n",
       "3  2603.379037  515.237299  0.091038  0.784454  0.860716   \n",
       "4  3294.162200  142.420213  0.507714  0.522686  0.904070   \n",
       "\n",
       "   j1 sqrt(tau^2_1)/tau^1_1  j1 n_trk       j1 pT1   j1 M_trim   j1 M_prun  \\\n",
       "0                  4.241889     136.0  1285.895950   18.881765    9.797733   \n",
       "1                  1.069972     320.0  1334.493332  556.665923  562.607897   \n",
       "2                  1.310040     332.0   678.557182  144.351550  142.366275   \n",
       "3                  1.102743     248.0  1284.020224  501.564320  506.727622   \n",
       "4                  1.853319     220.0  1087.658980  129.146700   36.160229   \n",
       "\n",
       "   ...  j2 sqrt(tau^2_1)/tau^1_1  j2 n_trk       j2 pT1   j2 M_trim  \\\n",
       "0  ...                  1.895988     128.0  1282.286017   42.162664   \n",
       "1  ...                  1.377217     348.0  1306.137883  395.226881   \n",
       "2  ...                  1.887494     236.0  1072.462085   54.235070   \n",
       "3  ...                  1.997360     352.0  1217.031950   81.842001   \n",
       "4  ...                  1.113248     204.0  1205.343324  103.456059   \n",
       "\n",
       "    j2 M_prun   j2 M_mmdt   j2 M_sdb1   j2 M_sdb2   j2 M_sdm1  isSignal  \n",
       "0   18.466533   18.466533   31.845136   42.162664    0.000000       0.0  \n",
       "1  393.309512  405.034096  405.034096  405.034096  405.034096       0.0  \n",
       "2   41.967840   41.352112   51.721630   70.442364   -0.000003       0.0  \n",
       "3   60.307703   60.307703   72.423677   84.480859    0.000003       0.0  \n",
       "4   99.817788  103.456059  103.456059  103.456059    0.000008       1.0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_rnd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == 'ROC':\n",
    "    dt_PureBkg = f_rnd.values\n",
    "else:\n",
    "    dt_PureBkg = f_PureBkg.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeInputs(dt_PureBkg):\n",
    "    \n",
    "    dt_PureBkg[:,1] = (dt_PureBkg[:,1]-np.mean(dt_PureBkg[:,1]))/np.std(dt_PureBkg[:,1])\n",
    "    dt_PureBkg[:,2] = (dt_PureBkg[:,2]-np.mean(dt_PureBkg[:,2]))/np.std(dt_PureBkg[:,2])\n",
    "    dt_PureBkg[:,3] = (dt_PureBkg[:,3]-np.mean(dt_PureBkg[:,3]))/np.std(dt_PureBkg[:,3])\n",
    "    dt_PureBkg[:,4] = (dt_PureBkg[:,4]-np.mean(dt_PureBkg[:,4]))/np.std(dt_PureBkg[:,4])\n",
    "    dt_PureBkg[:,5] = (dt_PureBkg[:,5]-np.mean(dt_PureBkg[:,5]))/np.std(dt_PureBkg[:,5])\n",
    "    dt_PureBkg[:,6] = (dt_PureBkg[:,6]-np.mean(dt_PureBkg[:,6]))/np.std(dt_PureBkg[:,6])\n",
    "\n",
    "\n",
    "    dt_PureBkg[:,8] = (dt_PureBkg[:,8]-np.mean(dt_PureBkg[:,8]))/np.std(dt_PureBkg[:,8])\n",
    "    dt_PureBkg[:,9] = (dt_PureBkg[:,9]-np.mean(dt_PureBkg[:,9]))/np.std(dt_PureBkg[:,9])\n",
    "    dt_PureBkg[:,10] = (dt_PureBkg[:,10]-np.mean(dt_PureBkg[:,10]))/np.std(dt_PureBkg[:,10])\n",
    "    dt_PureBkg[:,11] = (dt_PureBkg[:,11]-np.mean(dt_PureBkg[:,11]))/np.std(dt_PureBkg[:,11])\n",
    "    dt_PureBkg[:,12] = (dt_PureBkg[:,12]-np.mean(dt_PureBkg[:,12]))/np.std(dt_PureBkg[:,12])\n",
    "\n",
    "    dt_PureBkg[:,14] = (dt_PureBkg[:,14]-np.mean(dt_PureBkg[:,14]))/np.std(dt_PureBkg[:,14])\n",
    "    dt_PureBkg[:,15] = (dt_PureBkg[:,15]-np.mean(dt_PureBkg[:,15]))/np.std(dt_PureBkg[:,15])\n",
    "    dt_PureBkg[:,16] = (dt_PureBkg[:,16]-np.mean(dt_PureBkg[:,16]))/np.std(dt_PureBkg[:,16])\n",
    "    dt_PureBkg[:,17] = (dt_PureBkg[:,17]-np.mean(dt_PureBkg[:,17]))/np.std(dt_PureBkg[:,17])\n",
    "    dt_PureBkg[:,18] = (dt_PureBkg[:,18]-np.mean(dt_PureBkg[:,18]))/np.std(dt_PureBkg[:,18])\n",
    "    dt_PureBkg[:,19] = (dt_PureBkg[:,19]-np.mean(dt_PureBkg[:,19]))/np.std(dt_PureBkg[:,19])\n",
    "\n",
    "\n",
    "    dt_PureBkg[:,21] = (dt_PureBkg[:,21]-np.mean(dt_PureBkg[:,21]))/np.std(dt_PureBkg[:,21])\n",
    "    dt_PureBkg[:,22] = (dt_PureBkg[:,22]-np.mean(dt_PureBkg[:,22]))/np.std(dt_PureBkg[:,22])\n",
    "    dt_PureBkg[:,23] = (dt_PureBkg[:,23]-np.mean(dt_PureBkg[:,23]))/np.std(dt_PureBkg[:,23])\n",
    "    dt_PureBkg[:,24] = (dt_PureBkg[:,24]-np.mean(dt_PureBkg[:,24]))/np.std(dt_PureBkg[:,24])\n",
    "    dt_PureBkg[:,25] = (dt_PureBkg[:,25]-np.mean(dt_PureBkg[:,25]))/np.std(dt_PureBkg[:,25])\n",
    "    \n",
    "    return dt_PureBkg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.57757190e+03 9.86772703e+01 5.28903464e-01 7.88280821e-01\n",
      " 9.04470999e-01 4.24188929e+00 1.36000000e+02 1.28589595e+03\n",
      " 1.88817651e+01 9.79773305e+00 5.44031844e-01 9.79773305e+00\n",
      " 9.37639706e+01 0.00000000e+00 5.35190226e+01 6.68561818e-01\n",
      " 7.35744913e-01 7.55674293e-01 1.89598850e+00 1.28000000e+02\n",
      " 1.28228602e+03 4.21626640e+01 1.84665325e+01 1.84665325e+01\n",
      " 3.18451364e+01 4.21626640e+01 0.00000000e+00 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(dt_PureBkg[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dt_PureBkg = NormalizeInputs(dt_PureBkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1100000, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_PureBkg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = dt_PureBkg[:,27]\n",
    "bkg_idx = np.where(idx==0)[0]\n",
    "signal_idx = np.where(idx==1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_PureBkg = torch.tensor(dt_PureBkg[bkg_idx])\n",
    "total_PureBkg_train_x_1 = total_PureBkg.t()[1:7].t()\n",
    "total_PureBkg_train_x_2 = total_PureBkg.t()[8:13].t()\n",
    "total_PureBkg_train_x_3 = total_PureBkg.t()[14:20].t()\n",
    "total_PureBkg_train_x_4 = total_PureBkg.t()[21:26].t()\n",
    "\n",
    "#total_PureBkg_selection = torch.cat((total_PureBkg_train_x_1,total_PureBkg_train_x_2,total_PureBkg_train_x_3,total_PureBkg_train_x_4),dim=1)\n",
    "total_PureBkg_selection = torch.cat((total_PureBkg_train_x_1,total_PureBkg_train_x_3),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000000, 12])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_PureBkg_selection.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 500\n",
    "bkgAE_train_iterator = utils.DataLoader(total_PureBkg_selection, batch_size=bs, shuffle=True) \n",
    "bkgAE_test_iterator = utils.DataLoader(total_PureBkg_selection, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_NF(nn.Module):\n",
    "    def __init__(self, K, D):\n",
    "        super().__init__()\n",
    "        self.dim = D\n",
    "        self.K = K\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(12, 48),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(48, 30),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(30, 20),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(20, D * 2)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(D, 20),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(20, 30),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(30, 48),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(48, 12),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        flow_init = Radial(dim=D)\n",
    "        flows_init = [flow_init for _ in range(K)]\n",
    "        prior = MultivariateNormal(torch.zeros(D).cuda(), torch.eye(D).cuda())\n",
    "        self.flows = NormalizingFlowModel(prior, flows_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run Encoder and get NF params\n",
    "        enc = self.encoder(x)\n",
    "        mu = enc[:, :self.dim]\n",
    "        log_var = enc[:, self.dim: self.dim * 2]\n",
    "\n",
    "        # Re-parametrize\n",
    "        sigma = (log_var * .5).exp()\n",
    "        z = mu + sigma * torch.randn_like(sigma)\n",
    "        kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        # Construct more expressive posterior with NF\n",
    "        \n",
    "        z_k, _, sum_ladj = self.flows(z)\n",
    "        \n",
    "        kl_div = kl_div / x.size(0) - sum_ladj.mean()  # mean over batch\n",
    "\n",
    "        # Run Decoder\n",
    "        x_prime = self.decoder(z_k)\n",
    "        return x_prime, kl_div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Instance¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000\n",
    "N_EPOCHS = 50\n",
    "PRINT_INTERVAL = 2000\n",
    "NUM_WORKERS = 4\n",
    "LR = 1e-4\n",
    "\n",
    "N_FLOWS = 2\n",
    "Z_DIM = 2\n",
    "\n",
    "n_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE_NF(N_FLOWS, Z_DIM).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    global n_steps\n",
    "    train_loss = []\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, x in enumerate(bkgAE_train_iterator):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        x = x.float().cuda()\n",
    "\n",
    "        x_tilde, kl_div = model(x)\n",
    "        loss_recons = F.binary_cross_entropy(x_tilde, x, size_average=False) / x.size(0)\n",
    "        loss = loss_recons + kl_div\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append([loss_recons.item(), kl_div.item()])\n",
    "\n",
    "        if (batch_idx + 1) % PRINT_INTERVAL == 0:\n",
    "            print('\\tIter [{}/{} ({:.0f}%)]\\tLoss: {} Time: {:5.3f} ms/batch'.format(\n",
    "                batch_idx * len(x), 50000,\n",
    "                PRINT_INTERVAL * batch_idx / 50000,\n",
    "                np.asarray(train_loss)[-PRINT_INTERVAL:].mean(0),\n",
    "                1000 * (time.time() - start_time)\n",
    "            ))\n",
    "\n",
    "        n_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(split='valid'):\n",
    "    global n_steps\n",
    "    start_time = time.time()\n",
    "    val_loss = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, x in enumerate(bkgAE_test_iterator):\n",
    "            \n",
    "            x = x.float().cuda()\n",
    "\n",
    "            x_tilde, kl_div = model(x)\n",
    "            loss_recons = F.binary_cross_entropy(x_tilde, x, size_average=False) / x.size(0)\n",
    "            loss = loss_recons + kl_div\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "    print('\\nEvaluation Completed ({})!\\tLoss: {:5.4f} Time: {:5.3f} s'.format(\n",
    "        split,\n",
    "        np.asarray(val_loss).mean(0),\n",
    "        time.time() - start_time\n",
    "    ))\n",
    "    return np.asarray(val_loss).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark/miniconda3/envs/myenv/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tIter [999500/50000 (80%)]\tLoss: [-57.23885371   3.95649374] Time: 5.808 ms/batch\n",
      "\n",
      "Evaluation Completed (valid)!\tLoss: -94.6673 Time: 4.640 s\n",
      "Saving model!\n",
      "Epoch 2:\n",
      "\tIter [999500/50000 (80%)]\tLoss: [-106.365512      3.88438796] Time: 5.654 ms/batch\n",
      "\n",
      "Evaluation Completed (valid)!\tLoss: -112.1045 Time: 4.628 s\n",
      "Saving model!\n",
      "Epoch 3:\n",
      "\tIter [999500/50000 (80%)]\tLoss: [-117.07988034    3.85066593] Time: 5.839 ms/batch\n",
      "\n",
      "Evaluation Completed (valid)!\tLoss: -114.1918 Time: 4.709 s\n",
      "Saving model!\n",
      "Epoch 4:\n",
      "\tIter [999500/50000 (80%)]\tLoss: [-117.68609015    3.81492019] Time: 5.807 ms/batch\n",
      "\n",
      "Evaluation Completed (valid)!\tLoss: -113.9913 Time: 4.676 s\n",
      "Not saving model! Last saved: 3\n",
      "Epoch 5:\n",
      "\tIter [999500/50000 (80%)]\tLoss: [-117.85736625    3.79267807] Time: 5.650 ms/batch\n",
      "\n",
      "Evaluation Completed (valid)!\tLoss: -114.1419 Time: 4.644 s\n",
      "Not saving model! Last saved: 3\n",
      "Epoch 6:\n",
      "\tIter [999500/50000 (80%)]\tLoss: [-117.96274237    3.78161649] Time: 5.680 ms/batch\n",
      "\n",
      "Evaluation Completed (valid)!\tLoss: -113.8465 Time: 4.636 s\n",
      "Not saving model! Last saved: 3\n",
      "Epoch 7:\n",
      "\tIter [999500/50000 (80%)]\tLoss: [-118.06923428    3.76831683] Time: 5.701 ms/batch\n",
      "\n",
      "Evaluation Completed (valid)!\tLoss: -114.3265 Time: 4.646 s\n",
      "Saving model!\n",
      "Epoch 8:\n",
      "\tIter [999500/50000 (80%)]\tLoss: [-118.13113037    3.76384521] Time: 5.661 ms/batch\n",
      "\n",
      "Evaluation Completed (valid)!\tLoss: -114.4000 Time: 4.646 s\n",
      "Saving model!\n",
      "Epoch 9:\n",
      "\tIter [999500/50000 (80%)]\tLoss: [-118.11704468    3.75028216] Time: 5.677 ms/batch\n",
      "\n",
      "Evaluation Completed (valid)!\tLoss: -114.5362 Time: 4.655 s\n",
      "Saving model!\n",
      "Epoch 10:\n",
      "\tIter [999500/50000 (80%)]\tLoss: [-118.17792665    3.7419112 ] Time: 5.693 ms/batch\n",
      "\n",
      "Evaluation Completed (valid)!\tLoss: -114.3031 Time: 4.649 s\n",
      "Not saving model! Last saved: 9\n",
      "Epoch 11:\n"
     ]
    }
   ],
   "source": [
    "BEST_LOSS = 99999\n",
    "LAST_SAVED = -1\n",
    "PATIENCE_COUNT = 0\n",
    "PATIENCE_LIMIT = 5\n",
    "for epoch in range(1, N_EPOCHS):\n",
    "    print(\"Epoch {}:\".format(epoch))\n",
    "    train()\n",
    "    cur_loss = evaluate()\n",
    "\n",
    "    if cur_loss <= BEST_LOSS:\n",
    "        PATIENCE_COUNT = 0\n",
    "        BEST_LOSS = cur_loss\n",
    "        LAST_SAVED = epoch\n",
    "        print(\"Saving model!\")\n",
    "        if mode == 'ROC':\n",
    "            torch.save(model.state_dict(),\"/data/t3home000/spark/QUASAR/weights/bkg_vae_NF_radial_RND.h5\")\n",
    "        else:\n",
    "            torch.save(model.state_dict(), \"/data/t3home000/spark/QUASAR/weights/bkg_vae_NF_radial_PureBkg.h5\")\n",
    "    else:\n",
    "        PATIENCE_COUNT += 1\n",
    "        print(\"Not saving model! Last saved: {}\".format(LAST_SAVED))\n",
    "        if PATIENCE_COUNT > 5:\n",
    "            print(\"Patience Limit Reached\")\n",
    "            break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"/data/t3home000/spark/QUASAR/weights/bkg_vae_NF_radial_RND.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mass_and_loss(inputstring):\n",
    "    f_in = pd.read_hdf(inputstring)\n",
    "    dt_in = f_in.values\n",
    "    dt_in[:,1] = (dt_in[:,1]-np.mean(dt_in[:,1]))/np.std(dt_in[:,1])\n",
    "    dt_in[:,2] = (dt_in[:,2]-np.mean(dt_in[:,2]))/np.std(dt_in[:,2])\n",
    "    dt_in[:,3] = (dt_in[:,3]-np.mean(dt_in[:,3]))/np.std(dt_in[:,3])\n",
    "    dt_in[:,4] = (dt_in[:,4]-np.mean(dt_in[:,4]))/np.std(dt_in[:,4])\n",
    "    dt_in[:,5] = (dt_in[:,5]-np.mean(dt_in[:,5]))/np.std(dt_in[:,5])\n",
    "    dt_in[:,6] = (dt_in[:,6]-np.mean(dt_in[:,6]))/np.std(dt_in[:,6])\n",
    "\n",
    "    dt_in[:,8] = (dt_in[:,8]-np.mean(dt_in[:,8]))/np.std(dt_in[:,8])\n",
    "    dt_in[:,9] = (dt_in[:,9]-np.mean(dt_in[:,9]))/np.std(dt_in[:,9])\n",
    "    dt_in[:,10] = (dt_in[:,10]-np.mean(dt_in[:,10]))/np.std(dt_in[:,10])\n",
    "    dt_in[:,11] = (dt_in[:,11]-np.mean(dt_in[:,11]))/np.std(dt_in[:,11])\n",
    "    dt_in[:,12] = (dt_in[:,12]-np.mean(dt_in[:,12]))/np.std(dt_in[:,12])\n",
    "\n",
    "    dt_in[:,14] = (dt_in[:,14]-np.mean(dt_in[:,14]))/np.std(dt_in[:,14])\n",
    "    dt_in[:,15] = (dt_in[:,15]-np.mean(dt_in[:,15]))/np.std(dt_in[:,15])\n",
    "    dt_in[:,16] = (dt_in[:,16]-np.mean(dt_in[:,16]))/np.std(dt_in[:,16])\n",
    "    dt_in[:,17] = (dt_in[:,17]-np.mean(dt_in[:,17]))/np.std(dt_in[:,17])\n",
    "    dt_in[:,18] = (dt_in[:,18]-np.mean(dt_in[:,18]))/np.std(dt_in[:,18])\n",
    "    dt_in[:,19] = (dt_in[:,19]-np.mean(dt_in[:,19]))/np.std(dt_in[:,19])\n",
    "    \n",
    "    dt_in[:,21] = (dt_in[:,21]-np.mean(dt_in[:,21]))/np.std(dt_in[:,21])\n",
    "    dt_in[:,22] = (dt_in[:,22]-np.mean(dt_in[:,22]))/np.std(dt_in[:,22])\n",
    "    dt_in[:,23] = (dt_in[:,23]-np.mean(dt_in[:,23]))/np.std(dt_in[:,23])\n",
    "    dt_in[:,24] = (dt_in[:,24]-np.mean(dt_in[:,24]))/np.std(dt_in[:,24])\n",
    "    dt_in[:,25] = (dt_in[:,25]-np.mean(dt_in[:,25]))/np.std(dt_in[:,25])   \n",
    "    \n",
    "    total_in = torch.tensor(dt_in)\n",
    "    total_in_train_x_1 = total_in.t()[1:7].t()\n",
    "    total_in_train_x_2 = total_in.t()[8:13].t()\n",
    "    total_in_train_x_3 = total_in.t()[14:20].t()\n",
    "    total_in_train_x_4 = total_in.t()[21:26].t()\n",
    "    total_in_selection = torch.cat((total_in_train_x_1,total_in_train_x_2,total_in_train_x_3,total_in_train_x_4),dim=1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loss_total_in = torch.mean((model(total_in_selection.float().cuda())[0]- total_in_selection.float().cuda())**2,dim=1).data.cpu().numpy()\n",
    "    \n",
    "    f_in = pd.read_hdf(inputstring)\n",
    "    dt_in = f_in.values\n",
    "    \n",
    "    return dt_in[:,0], dt_in[:,10], dt_in[:,23], dt_in[:,9], dt_in[:,22], loss_total_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mass(inputstring):\n",
    "\n",
    "    f_in = pd.read_hdf(inputstring)\n",
    "    dt_in = f_in.values\n",
    "    \n",
    "    return dt_in[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb2mass = get_mass(\"../../../2_lhc/LHC_Olympics2020/processing/test_dataset/MassRatio_BB1.h5\")\n",
    "purebkgmass = get_mass(\"../../../2_lhc/LHC_Olympics2020/processing/test_dataset/MassRatio_pureBkg.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb2mass, bb2mmdt1, bb2mmdt2, bb2prun1,bb2prun2, bb2loss = get_mass_and_loss(\"../../../2_lhc/LHC_Olympics2020/processing/test_dataset/MassRatio_BB1.h5\")\n",
    "purebkgmass, purebkgmmdt1, purebkgmmdt2, purebkgprun1,purebkgprun2, purebkgloss = get_mass_and_loss(\"../../../2_lhc/LHC_Olympics2020/processing/test_dataset/MassRatio_pureBkg.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "bins = np.linspace(0,5,1100)\n",
    "plt.hist(bb2loss,bins=bins,alpha=0.3,color='b',label='blackbox1')\n",
    "plt.hist(purebkgloss,bins=bins,alpha=0.3,color='r',label='background')\n",
    "plt.xlabel(r'Autoencoder Loss')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(dt_in):\n",
    "\n",
    "    #dt_in[:,1] = (dt_in[:,1]-np.mean(dt_in[:,1]))/np.std(dt_in[:,1])\n",
    "    #dt_in[:,2] = (dt_in[:,2]-np.mean(dt_in[:,2]))/np.std(dt_in[:,2])\n",
    "    #dt_in[:,3] = (dt_in[:,3]-np.mean(dt_in[:,3]))/np.std(dt_in[:,3])\n",
    "    #dt_in[:,4] = (dt_in[:,4]-np.mean(dt_in[:,4]))/np.std(dt_in[:,4])\n",
    "    #dt_in[:,5] = (dt_in[:,5]-np.mean(dt_in[:,5]))/np.std(dt_in[:,5])\n",
    "    #dt_in[:,6] = (dt_in[:,6]-np.mean(dt_in[:,6]))/np.std(dt_in[:,6])\n",
    "\n",
    "    #dt_in[:,8] = (dt_in[:,8]-np.mean(dt_in[:,8]))/np.std(dt_in[:,8])\n",
    "    #dt_in[:,9] = (dt_in[:,9]-np.mean(dt_in[:,9]))/np.std(dt_in[:,9])\n",
    "    #dt_in[:,10] = (dt_in[:,10]-np.mean(dt_in[:,10]))/np.std(dt_in[:,10])\n",
    "    #dt_in[:,11] = (dt_in[:,11]-np.mean(dt_in[:,11]))/np.std(dt_in[:,11])\n",
    "    #dt_in[:,12] = (dt_in[:,12]-np.mean(dt_in[:,12]))/np.std(dt_in[:,12])\n",
    "\n",
    "    #dt_in[:,14] = (dt_in[:,14]-np.mean(dt_in[:,14]))/np.std(dt_in[:,14])\n",
    "    #dt_in[:,15] = (dt_in[:,15]-np.mean(dt_in[:,15]))/np.std(dt_in[:,15])\n",
    "    #dt_in[:,16] = (dt_in[:,16]-np.mean(dt_in[:,16]))/np.std(dt_in[:,16])\n",
    "    #dt_in[:,17] = (dt_in[:,17]-np.mean(dt_in[:,17]))/np.std(dt_in[:,17])\n",
    "    #dt_in[:,18] = (dt_in[:,18]-np.mean(dt_in[:,18]))/np.std(dt_in[:,18])\n",
    "    #dt_in[:,19] = (dt_in[:,19]-np.mean(dt_in[:,19]))/np.std(dt_in[:,19])\n",
    "    \n",
    "    #dt_in[:,21] = (dt_in[:,21]-np.mean(dt_in[:,21]))/np.std(dt_in[:,21])\n",
    "    #dt_in[:,22] = (dt_in[:,22]-np.mean(dt_in[:,22]))/np.std(dt_in[:,22])\n",
    "    #dt_in[:,23] = (dt_in[:,23]-np.mean(dt_in[:,23]))/np.std(dt_in[:,23])\n",
    "    #dt_in[:,24] = (dt_in[:,24]-np.mean(dt_in[:,24]))/np.std(dt_in[:,24])\n",
    "    #dt_in[:,25] = (dt_in[:,25]-np.mean(dt_in[:,25]))/np.std(dt_in[:,25])   \n",
    "    \n",
    "    total_in = torch.tensor(dt_in)\n",
    "    total_in_train_x_1 = total_in.t()[1:7].t()\n",
    "    total_in_train_x_2 = total_in.t()[8:13].t()\n",
    "    total_in_train_x_3 = total_in.t()[14:20].t()\n",
    "    total_in_train_x_4 = total_in.t()[21:26].t()\n",
    "    #total_in_selection = torch.cat((total_in_train_x_1,total_in_train_x_2,total_in_train_x_3,total_in_train_x_4),dim=1)\n",
    "    total_in_selection = torch.cat((total_in_train_x_1,total_in_train_x_3),dim=1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loss_total_in = torch.mean((model(total_in_selection.float().cuda())[0]- total_in_selection.float().cuda())**2,dim=1).data.cpu().numpy()\n",
    "    \n",
    "    return loss_total_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.read_hdf(\"/data/t3home000/spark/LHCOlympics/data/MassRatio_RandD.h5\")\n",
    "dt = f.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = dt_PureBkg[:,27]\n",
    "bkg_idx = np.where(idx==0)[0]\n",
    "signal_idx = np.where(idx==1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_bkg = get_loss(dt_PureBkg[bkg_idx])\n",
    "loss_sig = get_loss(dt_PureBkg[signal_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "bins = np.linspace(0,5,1100)\n",
    "plt.hist(loss_bkg,bins=bins,alpha=0.3,color='b',label='bkg')\n",
    "plt.hist(loss_sig,bins=bins,alpha=0.3,color='r',label='sig')\n",
    "plt.xlabel(r'Autoencoder Loss')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tpr_fpr(sigloss,bkgloss,aetype='sig'):\n",
    "    bins = np.linspace(0,50,1001)\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    for cut in bins:\n",
    "        if aetype == 'sig':\n",
    "            tpr.append(np.where(sigloss<cut)[0].shape[0]/len(sigloss))\n",
    "            fpr.append(np.where(bkgloss<cut)[0].shape[0]/len(bkgloss))\n",
    "        if aetype == 'bkg':\n",
    "            tpr.append(np.where(sigloss>cut)[0].shape[0]/len(sigloss))\n",
    "            fpr.append(np.where(bkgloss>cut)[0].shape[0]/len(bkgloss))\n",
    "    return tpr,fpr   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision_recall(sigloss,bkgloss,aetype='bkg'):\n",
    "    bins = np.linspace(0,100,1001)\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    precision = []\n",
    "    for cut in bins:\n",
    "        if aetype == 'sig':\n",
    "            tpr.append(np.where(sigloss<cut)[0].shape[0]/len(sigloss))\n",
    "            precision.append((np.where(sigloss<cut)[0].shape[0])/(np.where(bkgloss<cut)[0].shape[0]+np.where(sigloss<cut)[0].shape[0]))\n",
    "            \n",
    "        if aetype == 'bkg':\n",
    "            tpr.append(np.where(sigloss>cut)[0].shape[0]/len(sigloss))\n",
    "            precision.append((np.where(sigloss>cut)[0].shape[0])/(np.where(bkgloss>cut)[0].shape[0]+np.where(sigloss>cut)[0].shape[0]))\n",
    "    return precision,tpr  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_tpr, bkg_fpr = get_tpr_fpr(loss_sig,loss_bkg,aetype='bkg')\n",
    "precision, recall = get_precision_recall(loss_sig,loss_bkg,aetype='bkg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOW_ARCH = 'Radial'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'NFLOWVAE_{FLOW_ARCH}_sigloss.npy',loss_sig)\n",
    "np.save(f'NFLOWVAE_{FLOW_ARCH}_bkgloss.npy',loss_bkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'NFLOWVAE_{FLOW_ARCH}_precision.npy',precision)\n",
    "np.save(f'NFLOWVAE_{FLOW_ARCH}_recall.npy',recall)\n",
    "np.save(f'NFLOWVAE_{FLOW_ARCH}_bkgAE_fpr.npy',bkg_fpr)\n",
    "np.save(f'NFLOWVAE_{FLOW_ARCH}_bkgAE_tpr.npy',bkg_tpr)\n",
    "np.save(f'NFLOWVAE_{FLOW_ARCH}_sigloss.npy',loss_sig)\n",
    "np.save(f'NFLOWVAE_{FLOW_ARCH}_bkgloss.npy',loss_bkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bkg_fpr,bkg_tpr,label='Bkg NFlowVAE-radial')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recall,precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
